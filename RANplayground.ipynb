{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df345a31-d947-463d-832d-68a5175e4a7d",
   "metadata": {},
   "source": [
    "Building the dataset of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ab51f-d954-42fa-bf20-80ea54b6757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STOP - ONLY if needed\n",
    "# Allows printing full text\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "#mid_keywords = best_keywords(data, 1, 0.49, 0.51)    # same as above, but for average papers\n",
    "#low_keywords = best_keywords(data, 1, 0.03, 0.05)    # same as above, but for poor papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fd4ad-322a-4f6e-af15-7194165b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PUT MAIN HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b81166-b7ea-4668-a3d2-c09b9a9b246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\nX_train_z = scaler.fit_transform(X_train)\\nX_val_z  =scaler.transform(X_val)\\n\\npolynomial_features = PolynomialFeatures(degree = 2)\\nx_train_poly = polynomial_features.fit_transform(X_train_z)\\nx_val_poly = polynomial_features.transform(X_val_z)\\n\\nmodel = LinearRegression()\\nmodel.fit(x_train_poly, y_train)\\ny_poly_pred = model.predict(x_val_poly)\\n\\nprint(r2_score(y_val, y_poly_pred))   # -0.04350391168707901\\nprint(mean_absolute_error(y_val, y_poly_pred))    # 32.65668266590838\\n\\nsource: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Machine Learning Challenge\n",
    "# Course: Machine Learning (880083-M-6)\n",
    "# Group 58\n",
    " \n",
    "##########################################\n",
    "#             Import packages            #\n",
    "##########################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import yake  #NOTE: with Anaconda: conda install -c conda-forge yake\n",
    "\n",
    "##########################################\n",
    "#      Import self-made functions        #\n",
    "##########################################\n",
    "from CODE.data_preprocessing.split_val import split_val\n",
    "from CODE.data_preprocessing.find_outliers_tukey import find_outliers_tukey\n",
    "\n",
    "#feature based on the title of the paper\n",
    "from CODE.features.length_title import length_title\n",
    "\n",
    "# features based on 'field_of_study' column \n",
    "from CODE.features.field_variety import field_variety         \n",
    "from CODE.features.field_popularity import field_popularity\n",
    "from CODE.features.field_citations_avarage import field_citations_avarage \n",
    "\n",
    "# features based on the topics of the paper\n",
    "from CODE.features.topic_citations_avarage import topic_citations_avarage\n",
    "from CODE.features.topic_variety import topics_variety\n",
    "from CODE.features.topic_popularity import topic_popularity\n",
    "from CODE.features.topic_citations_avarage import topic_citations_avarage\n",
    "\n",
    "# features based on the abstract of the paper\n",
    "from CODE.features.keywords import best_keywords\n",
    "from CODE.features.abst_words import abst_words\n",
    "from CODE.features.abst_words import abst_count\n",
    "\n",
    "# features based on the venue of the paper\n",
    "from CODE.features.venue_popularity import venue_popularity\n",
    "from CODE.features.venue_citations import venues_citations\n",
    "\n",
    "from CODE.features.age import age\n",
    "\n",
    "# features based on the authors of the paper\n",
    "from CODE.features.author_h_index import author_h_index\n",
    "from CODE.features.paper_h_index import paper_h_index\n",
    "from CODE.features.team_size import team_size\n",
    "from CODE.features.author_database import author_database\n",
    "\n",
    "\n",
    "##########################################\n",
    "#              Load datasets             #\n",
    "##########################################\n",
    "# Main datasets\n",
    "data = pd.read_json('DATA/train.json')      # Training set\n",
    "test = pd.read_json('DATA/test.json')       # Test set\n",
    "\n",
    "# Author-centric datasets\n",
    "#   These datasets were made using our self-made functions 'citations_per_author' (for the author_citation_dic)\n",
    "#   These functions took a long time to make (ballpark ~10 minutes on a laptop in 'silent mode'), so instead we \n",
    "#   decided to run this function once, save the data, and reload the datasets instead of running the function again. \n",
    "import pickle\n",
    "with open('my_dataset1.pickle', 'rb') as dataset:\n",
    "    author_citation_dic = pickle.load(dataset)\n",
    "with open('my_dataset2.pickle', 'rb') as dataset2:\n",
    "    author_db = pickle.load(dataset2)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#        Missing values handling         #\n",
    "##########################################\n",
    "\n",
    "# Missing values for feature 'fields_of_study'\n",
    "data.loc[data['fields_of_study'].isnull(), 'fields_of_study'] = \"\"\n",
    "\n",
    "# Missing values for feature 'title'\n",
    "data.loc[data['title'].isnull(), 'title'] = \"\"\n",
    "\n",
    "# Missing values for feature 'abstract'\n",
    "data.loc[data['abstract'].isnull(), 'abstract'] = \"\"\n",
    "    \n",
    "# Missing values for features 'authors'\n",
    "data.loc[data['authors'].isnull(), 'authors'] = \"\"\n",
    "\n",
    "# Missing values for feature 'venue'\n",
    "data.loc[data['venue'].isnull(), 'venue'] = \"\"\n",
    "    \n",
    "# Missing values for feature 'year'\n",
    "# data.loc[data['fields_of_study'].isnull(), 'fields_of_study'] = mean(year) \n",
    "        #   Take mean by venue instead\n",
    "        #       If venue not known, take something else?\n",
    "\n",
    "# Missing values for feature 'references'\n",
    "data.loc[data['references'].isnull(), 'references'] = \"\"\n",
    "\n",
    "# Missing values for feature 'topics'\n",
    "data.loc[data['topics'].isnull(), 'topics'] = \"\"\n",
    "\n",
    "# Missing values for feature 'is_open_access'\n",
    "#data.loc[data['is_open_access'].isnull(), 'is_open_access'] = \"\" \n",
    "        #   Take most frequent occurrence for venue\n",
    "        #       If venue not known, do something else?\n",
    "    \n",
    "##########################################\n",
    "#       Create basic numeric df          #\n",
    "##########################################\n",
    "end = len(data)\n",
    "num_X = data.loc[ 0:end+1 , ('doi', 'citations', 'year', 'references') ]  ##REMOVE DOI\n",
    "\n",
    "\n",
    "##########################################\n",
    "#            Feature creation            #\n",
    "##########################################\n",
    "\"\"\"\n",
    "FEATURE DATAFRAME: num_X\n",
    "\n",
    "ALL: After writing a funtion to create a feature, please incorporate your new feature as a column on the dataframe below.\n",
    "This is the dataframe we will use to train the models.\n",
    "\n",
    "DO NOT change the order in this section if at all possible\n",
    "\"\"\"\n",
    "num_X['title_length'] = length_title(data)      # returns a numbered series\n",
    "num_X['field_variety'] = field_variety(data)    # returns a numbered series \n",
    "num_X['field_popularity'] = field_popularity(data) # returns a numbered series\n",
    "num_X['field_citations_avarage'] = field_citations_avarage(data) # returns a numbered series\n",
    "num_X['team_sz'] = team_size(data)           # returns a numbered series\n",
    "num_X['topic_var'] = topics_variety(data)    # returns a numbered series\n",
    "num_X['topic_popularity'] = topic_popularity(data) # returns a numbered series\n",
    "num_X['topic_citations_avarage'] = topic_citations_avarage(data) # returns a numbered series\n",
    "num_X['venue_popularity'], num_X['venue'] = venue_popularity(data)  # returns a numbered series and a pandas.Series of the 'venues' column reformatted \n",
    "num_X['open_access'] = pd.get_dummies(data[\"is_open_access\"], drop_first = True)  # returns pd.df (True = 1)\n",
    "num_X['age'] = age(data)               # returns a numbered series. Needs to be called upon AFTER the venues have been reformed (from venue_frequency)\n",
    "num_X['venPresL'] = venues_citations(data)   # returns a numbered series. Needs to be called upon AFTER the venues have been reformed (from venue_frequency)\n",
    "keywords = best_keywords(data, 1, 0.954, 0.955)    # from [data set] get [integer] keywords from papers btw [lower bound] and [upper bound] quantiles; returns list\n",
    "num_X['has_keyword'] = abst_words(data, keywords)#returns a numbered series: 1 if any of the words is present in the abstract, else 0\n",
    "num_X['keyword_count'] = abst_count(data, keywords) # same as above, only a count (noot bool)\n",
    "\n",
    "# Author H-index\n",
    "author_db, reformatted_authors = author_database(data)\n",
    "data['authors'] = reformatted_authors\n",
    "num_X['h_index'] = paper_h_index(data, author_citation_dic) # Returns a numbered series. Must come after author names have been reformatted.\n",
    "\n",
    "field_avg_cit = num_X.groupby('field_variety').citations.mean()\n",
    "for field, field_avg in zip(field_avg_cit.index, field_avg_cit):\n",
    "    num_X.loc[num_X['field_variety'] == field, 'field_cit'] = field_avg\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "END do not reorder\n",
    "\"\"\"\n",
    "\n",
    "##########################################\n",
    "#    Deal with specific missing values   #\n",
    "##########################################\n",
    "# Open_access, thanks to jreback (27th of July 2016) https://github.com/pandas-dev/pandas/issues/13809\n",
    "OpAc_by_venue = num_X.groupby('venue').open_access.apply(lambda x: x.mode()) # Take mode for each venue\n",
    "OpAc_by_venue = OpAc_by_venue.to_dict()\n",
    "missing_OpAc = num_X.loc[num_X['open_access'].isnull(),]\n",
    "for i, i_paper in missing_OpAc.iterrows():\n",
    "    venue = i_paper['venue']\n",
    "    doi = i_paper['doi']\n",
    "    index = num_X[num_X['doi'] == doi].index[0]\n",
    "    if venue in OpAc_by_venue.keys():   # If a known venue, append the most frequent value for that venue\n",
    "        num_X.loc[index,'open_access'] = OpAc_by_venue[venue] # Set most frequent occurrence \n",
    "    else:                               # Else take most occurring value in entire dataset\n",
    "        num_X.loc[index,'open_access'] = num_X.open_access.mode()[0] # Thanks to BENY (2nd of February, 2018) https://stackoverflow.com/questions/48590268/pandas-get-the-most-frequent-values-of-a-column\n",
    "\n",
    "# Year\n",
    "year_by_venue = num_X.groupby('venue').year.apply(lambda x: x.mean()) # Take mean for each venue\n",
    "year_by_venue = year_by_venue.to_dict()\n",
    "missing_year = num_X.loc[num_X['year'].isnull(),]\n",
    "for i, i_paper in missing_year.iterrows():\n",
    "    venue = i_paper['venue']\n",
    "    doi = i_paper['doi']\n",
    "    index = num_X[num_X['doi'] == doi].index[0]\n",
    "    if venue in year_by_venue.keys():   # If a known venue, append the mean value for that venue\n",
    "        num_X.loc[index, 'year'] = year_by_venue[venue] # Set mean publication year\n",
    "    else:                               # Else take mean value of entire dataset\n",
    "        num_X.loc[index,'year'] = num_X.year.mean()\n",
    "      \n",
    "### Drop columns containing just strings\n",
    "num_X = num_X.drop(['venue', 'doi', 'field_variety'], axis = 1)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#           Outlier detection 1          #\n",
    "##########################################\n",
    "# 9658 rows in the full num_X\n",
    "# 9494 rows with all turned on\n",
    "\n",
    "num_X = num_X[num_X['references'] < 500]\n",
    "num_X = num_X[num_X['team_sz'] < 40]\n",
    "num_X = num_X[num_X['topic_var'] < 60]\n",
    "num_X = num_X[num_X['venPresL'] < 300]\n",
    "num_X = num_X[num_X['h_index'] < 30]\n",
    "\n",
    "\n",
    "##########################################\n",
    "#            Train/val split             #\n",
    "##########################################\n",
    "\n",
    "## train/val split\n",
    "X_train, X_val, y_train, y_val = split_val(num_X, target_variable = 'citations')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "INSERT outlier detection on X_train here - ALBERT\n",
    "\"\"\"\n",
    "\n",
    "##########################################\n",
    "#           Outlier detection 2          #\n",
    "##########################################\n",
    "### MODEL code for outlier detection\n",
    "### names: X_train, X_val, y_train, y_val\n",
    "\n",
    "# print(list(X_train.columns))\n",
    "\n",
    "out_y = (find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))[0]\n",
    "out_rows = out_y\n",
    "\n",
    "# out_X = (find_outliers_tukey(x = X_train['team_sz'], top = 99, bottom = 0))[0]\n",
    "# out_rows = out_y + out_X\n",
    "\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "X_train = X_train.drop(labels = out_rows)\n",
    "y_train = y_train.drop(labels = out_rows)\n",
    "\n",
    "# Potential features to get rid of: team_sz; year and age are perfect correlates\n",
    "\n",
    "\n",
    "##########################################\n",
    "#         Model implementations          #\n",
    "##########################################\n",
    "\"\"\"\n",
    "IMPLEMENT models here\n",
    "NOTE: Please do not write over X_train, X_val, y_train, y_val in your model - make new variables if needed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#-----------simple regression, all columns\n",
    "\"\"\"\n",
    "simple_linear(X_train, y_train, X_val, y_val)\n",
    "\n",
    "MODEL RESULTS:\n",
    "R2: 0.03724\n",
    "MSE: 33.38996\n",
    "\"\"\"\n",
    "#-----------logistic regression, all columns\n",
    "\"\"\"\n",
    "log_reg(X_train, y_train, X_val, y_val)\n",
    "\n",
    "MODEL RESULTS:\n",
    "R2: 0.006551953988217396\n",
    "MSE: 34.07342328208346\n",
    "\"\"\"\n",
    "#-----------SGD regression, all columns\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.transform(X_val)\n",
    "y_ravel = np.ravel(y_train)\n",
    "lr = [ 1, .1, .01, .001, .0001]\n",
    "settings = []\n",
    "for learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "    for loss in ['squared_error', 'huber']:\n",
    "        for eta0 in lr:\n",
    "            model = SGDRegressor(learning_rate=learning_rate, eta0=eta0, loss=loss,random_state=666, max_iter=5000)\n",
    "            model.fit(X_train_z, y_ravel)\n",
    "            y_pred = model.predict(X_val_z)\n",
    "            \n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 =  r2_score(y_val, y_pred)\n",
    "            settings.append((learning_rate, eta0, loss, mae, r2))\n",
    "            print(settings[-1])\n",
    "\n",
    "# MODEL RESULTS:\n",
    "# Best outcome: ('constant', 0.01, 'squared_error', 35.74249957361433, 0.04476790061780822)\n",
    "\"\"\"\n",
    "\n",
    "#-----------polynomial regression, all columns\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.transform(X_val)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree = 2)\n",
    "x_train_poly = polynomial_features.fit_transform(X_train_z)\n",
    "x_val_poly = polynomial_features.transform(X_val_z)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_poly, y_train)\n",
    "y_poly_pred = model.predict(x_val_poly)\n",
    "\n",
    "print(r2_score(y_val, y_poly_pred))   # -0.04350391168707901\n",
    "print(mean_absolute_error(y_val, y_poly_pred))    # 32.65668266590838\n",
    "\n",
    "source: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.fit(X_train, y_train)\n",
    "#print('Best score: ', model.best_score_)\n",
    "#print('Best parameters: ', model.best_params_)\n",
    "#y_pred = model.predict(X_val)\n",
    "\n",
    "#from sklearn.metrics import r2_score\n",
    "#print(r2_score(y_val,y_pred))\n",
    "\n",
    "\n",
    "# import json\n",
    "#with open(\"sample.json\", \"w\") as outfile:\n",
    "    #json.dump(dictionary, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0219f-cab9-48a4-86f8-bd4be754b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------------ LETS EXPLORE!!! ------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04915fc1-7323-404b-aa26-761a7b9ee7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>year</th>\n",
       "      <th>references</th>\n",
       "      <th>title_length</th>\n",
       "      <th>field_popularity</th>\n",
       "      <th>field_citations_avarage</th>\n",
       "      <th>team_sz</th>\n",
       "      <th>topic_var</th>\n",
       "      <th>topic_popularity</th>\n",
       "      <th>topic_citations_avarage</th>\n",
       "      <th>venue_popularity</th>\n",
       "      <th>open_access</th>\n",
       "      <th>age</th>\n",
       "      <th>venPresL</th>\n",
       "      <th>has_keyword</th>\n",
       "      <th>keyword_count</th>\n",
       "      <th>h_index</th>\n",
       "      <th>field_cit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>39</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>45.186667</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>70.793257</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>44</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.134021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>344</td>\n",
       "      <td>43.519941</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.702479</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>57.829761</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.797101</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1131</td>\n",
       "      <td>60.718362</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.064516</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>8</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>230</td>\n",
       "      <td>57.869379</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9654</th>\n",
       "      <td>1</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>186</td>\n",
       "      <td>50.620347</td>\n",
       "      <td>462</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.042857</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9655</th>\n",
       "      <td>1</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.134021</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>3</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>586</td>\n",
       "      <td>2.764706</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.134021</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9657</th>\n",
       "      <td>228</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>38</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9394</td>\n",
       "      <td>37.902597</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1573</td>\n",
       "      <td>64.242391</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>70.793257</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.829665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9494 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      citations    year references  title_length  field_popularity  \\\n",
       "0            60  2015.0         39          10.0              9394   \n",
       "1             1  2020.0         44          18.0              9394   \n",
       "2             5  2017.0         30           8.0              9394   \n",
       "3             5  2017.0         11          13.0              9394   \n",
       "4            10  2015.0         26           5.0              9394   \n",
       "...         ...     ...        ...           ...               ...   \n",
       "9653          8  2014.0         25          10.0              9394   \n",
       "9654          1  2019.0         18           7.0              9394   \n",
       "9655          1  2021.0         12          16.0              9394   \n",
       "9656          3  2021.0         15          15.0               586   \n",
       "9657        228  2000.0         38           6.0              9394   \n",
       "\n",
       "      field_citations_avarage  team_sz  topic_var  topic_popularity  \\\n",
       "0                   37.902597        6          1                75   \n",
       "1                   37.902597        5          0                10   \n",
       "2                   37.902597        3          5               344   \n",
       "3                   37.902597        2          6              1019   \n",
       "4                   37.902597        2         23              1131   \n",
       "...                       ...      ...        ...               ...   \n",
       "9653                37.902597        4         11               230   \n",
       "9654                37.902597        4          3               186   \n",
       "9655                37.902597        2          0                10   \n",
       "9656                 2.764706        4          0                10   \n",
       "9657                37.902597        3          3              1573   \n",
       "\n",
       "      topic_citations_avarage  venue_popularity  open_access   age   venPresL  \\\n",
       "0                   45.186667              2005            1   6.0  70.793257   \n",
       "1                    4.134021                 8            1   1.0       3.75   \n",
       "2                   43.519941               116            1   4.0  12.702479   \n",
       "3                   57.829761                68            1   4.0  10.797101   \n",
       "4                   60.718362                30            1   6.0   4.064516   \n",
       "...                       ...               ...          ...   ...        ...   \n",
       "9653                57.869379                 9            1   7.0   3.444444   \n",
       "9654                50.620347               462            1   2.0   9.042857   \n",
       "9655                 4.134021                12            0   0.0   1.166667   \n",
       "9656                 4.134021                 9            0   0.0   1.777778   \n",
       "9657                64.242391              2005            1  21.0  70.793257   \n",
       "\n",
       "      has_keyword  keyword_count  h_index  field_cit  \n",
       "0               1             16     16.0  36.829665  \n",
       "1               1             25      1.0  36.829665  \n",
       "2               1             19      6.0  36.829665  \n",
       "3               1             14      5.0  36.829665  \n",
       "4               1             46      3.0  36.829665  \n",
       "...           ...            ...      ...        ...  \n",
       "9653            1             14      5.0  36.829665  \n",
       "9654            1             21      4.0  36.829665  \n",
       "9655            1             12      1.0  36.829665  \n",
       "9656            1             17      2.0   2.764706  \n",
       "9657            1             21      2.0  36.829665  \n",
       "\n",
       "[9494 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### FOR: exploring the new dataframe with numerical columns\n",
    "# --> NOTE: it would be more efficient to combine these first and only expand the df once (per addition type)\n",
    "\n",
    "num_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a982b8-4ac9-41e1-bfe2-5265e574b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: explore data train/val split  (should be 6470 train rows and 3188 validation rows)\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "print(\"number of keywords:\", len(keywords))\n",
    "print(\"total train rows:\", X_train.shape)\n",
    "print(\"numer w keyword:\", sum(X_train['has_keyword']))\n",
    "print()\n",
    "print(keywords)\n",
    "\n",
    "#X_val\n",
    "#y_train\n",
    "#y_val\n",
    "#6210 of 6313\n",
    "#6136 (of 6313) for 1 keyword from the top 1% of papers\n",
    "#4787 for 2 keywords from top .01% of papers (correlation: 0.036)\n",
    "#2917 for 1 keyword from top .01% of papers (correlation: 0.049)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828176-3b43-45e1-98ab-de2e109ee06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look at some correlations - full num_X\n",
    "\"\"\"\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "\n",
    "# From: https://www.kaggle.com/ankitjha/comparing-regression-models\n",
    "import seaborn as sns\n",
    "corr_mat = num_X.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be5a4a-20e6-45db-a477-548a15040c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look at some correlations - X_train\n",
    "NOTE: there is no y here\n",
    "\"\"\"\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "\n",
    "#temp = y_train hstack X_train\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/ankitjha/comparing-regression-models\n",
    "corr_mat = X_train.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c35830-e499-4409-a81b-e0fc78404744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------- LETS CODE!!! --------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c10f4baa-0dc1-4730-8d99-334b8206b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'references', 'title_length', 'field_popularity', 'field_citations_avarage', 'team_sz', 'topic_var', 'topic_popularity', 'topic_citations_avarage', 'venue_popularity', 'open_access', 'age', 'venPresL', 'has_keyword', 'keyword_count', 'h_index', 'field_cit']\n",
      "(6328, 17)\n",
      "(6328, 17)\n"
     ]
    }
   ],
   "source": [
    "play = X_train.copy()\n",
    "print(list(X_train.columns))\n",
    "print(X_train.shape)\n",
    "print(play.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b714df-fc32-4384-9f0a-7e83c55dd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CODE.data_preprocessing.outlier_threshold import outlier_threshold\n",
    "outlier_threshold(play, 'references', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbc5db0e-414e-4054-8a91-f6d8532aac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4185, 8819]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6140/401058170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mout_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_refs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mout_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "a = play[play['references'] > 500]\n",
    "out_refs = list(a.index)\n",
    "print(out_refs)\n",
    "print(type(out_refs))\n",
    "\n",
    "out_rows.append(out_refs)\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "play = play.drop(labels = out_rows)\n",
    "\n",
    "\n",
    "# play = play[play['team_sz'] < 40]\n",
    "\n",
    "# X_train = X_train[X_train['team_sz'] < 40]\n",
    "# X_train = X_train[X_train['topic_var'] < 60]\n",
    "# X_train = X_train[X_train['venPresL'] < 300]\n",
    "# X_train = X_train[X_train['h_index'] < 30]\n",
    "\n",
    "# y_train = y_train[y_train['references'] < 500]\n",
    "# y_train = y_train[y_train['team_sz'] < 40]\n",
    "# y_train = y_train[y_train['topic_var'] < 60]\n",
    "# y_train = y_train[y_train['venPresL'] < 300]\n",
    "# y_train = y_train[y_train['h_index'] < 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b111cba-bd2b-4180-9135-0f25a2f15e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6328, 17)\n",
      "(6328, 17)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(play.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b1c3b-c2c7-4fff-bfdb-7bb7335aceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose your columns\n",
    "\"\"\"\n",
    "\n",
    "#X_train_small = X_train.loc[ : , 'topic_var':'h_index'].copy()\n",
    "#X_val_small = X_val.loc[ : , 'topic_var':'h_index'].copy()\n",
    "\n",
    "drops = ['year', 'team_sz', 'has_keyword']\n",
    "X_train_small = X_train.copy()\n",
    "X_train_small.drop(drops, inplace = True, axis=1)\n",
    "\n",
    "X_val_small = X_val.copy()\n",
    "X_val_small.drop(drops, inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a95cc9-3e09-4144-a034-be9a57cd2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005b502-19ac-4427-b04e-deff7cde8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CODE.models.regression import simple_linear\n",
    "from CODE.models.regression import log_reg\n",
    "\n",
    "summaries = list(X_train.columns)\n",
    "print(summaries)\n",
    "\n",
    "for i in range(len(summaries)):\n",
    "#    fs = summaries[:i] + summaries[i+1:]\n",
    "    X_train_small = X_train.copy()\n",
    "    X_val_small = X_val.copy()\n",
    "    drops = summaries[i]\n",
    "    X_train_small.drop(drops, inplace = True, axis=1)\n",
    "    X_val_small.drop(drops, inplace = True, axis=1)\n",
    "\n",
    "    print(\"dropped:\", summaries[i])\n",
    "    \n",
    "    simple_linear(X_train_small, y_train, X_val_small, y_val)  #dropping venue_popularity helps a tiny bit\n",
    "#    log_reg(X_train_small, y_train, X_val_small, y_val)\n",
    "\n",
    "# log_reg\n",
    "# helps to drop: year, field_popularity, team_size, topic_var, age, has_keyword, keyword_count\n",
    "# hurts to drop: references, title length, topic_popularity, opic_citations_avarage, venue_popularity(!), \n",
    "# venPresL(!), h_index(!), field_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0da65-cd87-4ed8-a10a-dee5cd9f0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "playX = num_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fea05-1fd6-43b8-beb3-53b940d5c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playX = playX[playX['field_cit'] < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837298d1-8cfc-476b-b0e7-cc38b19a84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_X.plot.scatter(x=\"field_cit\", y=\"citations\", alpha=0.5) \n",
    "playX.plot.scatter(x=\"field_cit\", y=\"citations\", alpha=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ac0b1-fabe-4796-9a43-a2eb98fa1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(num_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e320317-d362-437c-b630-1a0ee9e8199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "#num_X.plot.scatter(x=\"year\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"references\", y=\"citations\", alpha=0.5)  # might have 3 outliers\n",
    "#num_X.plot.scatter(x=\"title_length\", y=\"citations\", alpha=0.5) \n",
    "#num_X.plot.scatter(x=\"team_sz\", y=\"citations\", alpha=0.5)  # might have 3 outliers\n",
    "#num_X.plot.scatter(x=\"topic_var\", y=\"citations\", alpha=0.5) # one outlier; maybe anything over 40\n",
    "#num_X.plot.scatter(x=\"topic_popularity\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"topic_citations_average\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"venue_popularity\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"open_access\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"age\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"venPresL\", y=\"citations\", alpha=0.5)  # anything over 300 as outlier?\n",
    "#num_X.plot.scatter(x=\"has_keyword\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"keyword_count\", y=\"citations\", alpha=0.5)\n",
    "#num_X.plot.scatter(x=\"h_index\", y=\"citations\", alpha=0.5)  # anything over 35 as outlier?\n",
    "#num_X.plot.scatter(x=\"field_cit\", y=\"citations\", alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de577ac-071b-4e2d-8153-bb16369a75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small\n",
    "#X_val_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c78afa-373a-41ed-8080-78d7eea535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abst_categories (the_data, keywords, mid_keywords, low_keywords):\n",
    "    abst = the_data['abstract']\n",
    "    counts = []\n",
    "    abst_key = []    \n",
    "\n",
    "    for i in abst:\n",
    "        if i == None:\n",
    "            abst_key.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            high = 0\n",
    "            for word in keywords:\n",
    "                if word in i.lower():\n",
    "                    high += 1\n",
    "            \n",
    "            mid = 0\n",
    "            for word in mid_keywords:\n",
    "                if word in i.lower():\n",
    "                    mid += 1\n",
    "\n",
    "            low = 0\n",
    "            for word in low_keywords:\n",
    "                if word in i.lower():\n",
    "                    low +=1\n",
    "\n",
    "        \n",
    "#        abst_key = np.argmax(abst_key)\n",
    "#        abst_key = (max(abst_key)).index\n",
    "\n",
    "\n",
    "    return pd.Series(abst_key)    \n",
    "    \n",
    "    \n",
    "print(sum(abst_categories (data, keywords, mid_keywords, low_keywords)))  #9499 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150ea0e-bae1-428c-8d1a-4f70df336c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove outliers\n",
    "NOTE: can't rerun this code without restarting the kernal\n",
    "\"\"\"\n",
    "#names: X_train, X_val, y_train, y_val\n",
    "#print(list(X_train.columns))\n",
    "\n",
    "# print(\"citations:\", find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))\n",
    "\n",
    "# print(\"year:\", find_outliers_tukey(X_train['year'], top = 74, bottom = 25))  # seems unnecessary\n",
    "# print(\"references:\", find_outliers_tukey(X_train['references'], top = 90, bottom = 10))  # seems unnecessary\n",
    "# print(\"team_size:\", find_outliers_tukey(X_train['team_size'], top = 99, bottom = 0))  # Meh\n",
    "# print(\"topic_variety:\", find_outliers_tukey(X_train['topic_variety'], top = 75, bottom = 10))  # not much diff btw top and normal\n",
    "# print(\"age:\", find_outliers_tukey(X_train['age'], top = 90, bottom = 10))  # Meh\n",
    "# print(\"open_access:\", find_outliers_tukey(X_train['open_access'], top = 100, bottom = 0))  # Not necessary: boolean\n",
    "# print(\"has_keyword:\", find_outliers_tukey(X_train['has_keyword'], top = 100, bottom = 0))  # Not necessary: boolean\n",
    "# print(\"title_length:\", find_outliers_tukey(X_train['title_length'], top = 90, bottom = 10))  # Meh\n",
    "# print(\"field_variety:\", find_outliers_tukey(X_train['field_variety'], top = 90, bottom = 10))  # seems unnecessary\n",
    "# print(\"venue_freq:\", find_outliers_tukey(X_train['venue_freq'], top = 90, bottom = 10))  # seems unnecessary\n",
    "\n",
    "\n",
    "out_y = (find_outliers_tukey(x = y_train['citations'], top = 95, bottom = 0))[0]\n",
    "#out_X = (find_outliers_tukey(x = X_train['team_size'], top = 99, bottom = 0))[0]\n",
    "out_rows = out_y\n",
    "#out_rows = out_y + out_X\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "\n",
    "print(\"X_train:\")\n",
    "print(X_train.shape)\n",
    "X_train = X_train.drop(labels = out_rows)\n",
    "print(X_train.shape)\n",
    "print()\n",
    "print(\"y_train:\")\n",
    "print(y_train.shape)\n",
    "y_train = y_train.drop(labels = out_rows)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcda6d7-a7e4-428b-bc05-3375fd7225e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2820d4b-4200-4214-bc23-27c872ae6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mini version of the main 'data' dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %pwd\n",
    "# %cd C:\\Users\\r_noc\\Desktop\\Python\\GIT\\machinelearning\n",
    "    \n",
    "play = data.sample(100, replace = False, axis = 0, random_state = 123)  \n",
    "\n",
    "\n",
    "print(play.shape)\n",
    "# print(play['abstract'])\n",
    "\n",
    "print(list(play.columns))\n",
    "# play['has_keyword'] = np.nan\n",
    "# print(play.shape)\n",
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a0f77-2b2e-41e5-a9fa-84705bf7fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c69f30-c1cb-445b-975f-69fd4e735485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train_small)\n",
    "X_val_z  =scaler.transform(X_val_small)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree = 2)\n",
    "x_train_poly = polynomial_features.fit_transform(X_train_z)\n",
    "x_val_poly = polynomial_features.transform(X_val_z)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_poly, y_train)\n",
    "y_poly_pred = model.predict(x_val_poly)\n",
    "\n",
    "print(r2_score(y_val, y_poly_pred))   # -0.04350391168707901\n",
    "print(mean_absolute_error(y_val, y_poly_pred))    # 32.65668266590838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc40789-f905-49ea-bb69-2bf8509828b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train_small)\n",
    "X_val_z  =scaler.transform(X_val_small)\n",
    "\n",
    "model = PolynomialFeatures(degree = 2)\n",
    "X_poly = model.fit_transform(X_train_z)\n",
    "model.fit(X_poly, y_train)\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_poly, y_train)\n",
    "\n",
    "y_pred_val = model2.predict(model.fit_transform(X_val_z))\n",
    "\n",
    "print(r2_score(y_val, y_pred_val))   #0.03724015197555319\n",
    "print(mean_absolute_error(y_val, y_pred_val))   #33.38996938585591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f0220-77fb-48f5-a02c-bb5f9b84219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#names: X_train, X_val, y_train, y_val\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train_small)\n",
    "X_val_z  =scaler.transform(X_val_small)\n",
    "y_ravel = np.ravel(y_train)\n",
    "lr = [ 1.1, 1, .1, .01, .001, .0001]\n",
    "settings = []\n",
    "for learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "    for loss in ['squared_error', 'huber']:\n",
    "        for eta0 in lr:\n",
    "            model = SGDRegressor(learning_rate=learning_rate, eta0=eta0, loss=loss,random_state=666, max_iter=5000)\n",
    "            model.fit(X_train_z, y_ravel)\n",
    "            y_pred = model.predict(X_val_z)\n",
    "            \n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 =  r2_score(y_val, y_pred)\n",
    "            settings.append((learning_rate, eta0, loss, mae, r2))\n",
    "            print(settings[-1])\n",
    "\n",
    "# Best outcome: ('constant', 0.01, 'squared_error', 35.74249957361433, 0.04476790061780822)\n",
    "# With small: ('invscaling', 1, 'squared_error', 48.92137807970932, 0.05128477811871335)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0fd2e-c1ef-486c-bfd4-c682c0901d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
