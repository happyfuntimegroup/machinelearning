{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df345a31-d947-463d-832d-68a5175e4a7d",
   "metadata": {},
   "source": [
    "Building the dataset of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fd4ad-322a-4f6e-af15-7194165b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PUT MAIN HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b81166-b7ea-4668-a3d2-c09b9a9b246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Challenge\n",
    "# Course: Machine Learning (880083-M-6)\n",
    "# Group 58\n",
    " \n",
    "##########################################\n",
    "#             Import packages            #\n",
    "##########################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import yake  #NOTE: with Anaconda: conda install -c conda-forge yake\n",
    "\n",
    "##########################################\n",
    "#      Import self-made functions        #\n",
    "##########################################\n",
    "from CODE.data_preprocessing.split_val import split_val\n",
    "from CODE.data_preprocessing.find_outliers_tukey import find_outliers_tukey\n",
    "from CODE.data_preprocessing.missing_values import missing_values1\n",
    "from CODE.data_preprocessing.missing_values import missing_values2\n",
    "\n",
    "#feature based on the title of the paper\n",
    "from CODE.features.length_title import length_title\n",
    "\n",
    "# features based on 'field_of_study' column \n",
    "from CODE.features.field_variety import field_variety         \n",
    "from CODE.features.field_popularity import field_popularity\n",
    "from CODE.features.field_citations_avarage import field_citations_avarage \n",
    "\n",
    "# features based on the topics of the paper\n",
    "from CODE.features.topic_citations_avarage import topic_citations_avarage\n",
    "from CODE.features.topic_variety import topics_variety\n",
    "from CODE.features.topic_popularity import topic_popularity\n",
    "from CODE.features.topic_citations_avarage import topic_citations_avarage\n",
    "\n",
    "# features based on the abstract of the paper\n",
    "from CODE.features.keywords import best_keywords\n",
    "from CODE.features.abst_words import abst_words\n",
    "from CODE.features.abst_words import abst_count\n",
    "\n",
    "# features based on the venue of the paper\n",
    "from CODE.features.venue_popularity import venue_popularity\n",
    "from CODE.features.venue_citations import venues_citations\n",
    "\n",
    "from CODE.features.age import age\n",
    "\n",
    "# features based on the authors of the paper\n",
    "from CODE.features.author_h_index import author_h_index\n",
    "from CODE.features.paper_h_index import paper_h_index\n",
    "from CODE.features.team_size import team_size\n",
    "from CODE.features.author_database import author_database\n",
    "print(\"Imports complete\")\n",
    "\n",
    "##########################################\n",
    "#              Load datasets             #\n",
    "##########################################\n",
    "# Main datasets\n",
    "data = pd.read_json('DATA/train.json')      # Training set\n",
    "test = pd.read_json('DATA/test.json')       # Test set\n",
    "\n",
    "# Author-centric datasets\n",
    "#   These datasets were made using our self-made functions 'citations_per_author' (for the author_citation_dic)\n",
    "#   These functions took a long time to make (ballpark ~10 minutes on a laptop in 'silent mode'), so instead we \n",
    "#   decided to run this function once, save the data, and reload the datasets instead of running the function again. \n",
    "import pickle\n",
    "with open('my_dataset1.pickle', 'rb') as dataset:\n",
    "    author_citation_dic = pickle.load(dataset)\n",
    "with open('my_dataset2.pickle', 'rb') as dataset2:\n",
    "    author_db = pickle.load(dataset2)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "##########################################\n",
    "#       drop na rows          #\n",
    "##########################################\n",
    "data.dropna(axis='index', how = 'any')  # not helpful over missing_values.\n",
    "\n",
    "##########################################\n",
    "#        Missing values handling         #\n",
    "##########################################\n",
    "missing_values1(data)  # keep this on even with dropna; should be redundant but it seems to be needed...\n",
    "missing_values1(test)  # this needs to stay on\n",
    "\n",
    "##########################################\n",
    "#       Create basic numeric df          #\n",
    "##########################################\n",
    "num_X = data.copy(deep = True)\n",
    "\n",
    "##########################################\n",
    "#            Feature creation            #\n",
    "##########################################\n",
    "\"\"\"\n",
    "FEATURE DATAFRAME: num_X\n",
    "\n",
    "ALL: After writing a funtion to create a feature, please incorporate your new feature as a column on the dataframe below.\n",
    "This is the dataframe we will use to train the models.\n",
    "\n",
    "DO NOT change the order in this section if at all possible\n",
    "\"\"\"\n",
    "num_X['title_length'] = length_title(data)      # returns a numbered series with wordlength of the title\n",
    "test['title_length'] = length_title(test)\n",
    "num_X['field_variety'] = field_variety(data)    # returns a numbered series with amount of fields\n",
    "test['field_variety'] = field_variety(test)    # returns a numbered series with amount of fields\n",
    "num_X['field_popularity'], test['field_popularity'] = field_popularity(data, test) # returns a numbered series with \n",
    "num_X['field_citations_avarage'], test['field_citations_avarage']  = field_citations_avarage(data, test) # returns a numbered series\n",
    "num_X['team_sz'] = team_size(data)           # returns a numbered series\n",
    "test['team_sz'] = team_size(test)           # returns a numbered series\n",
    "num_X['topic_variety'] = topics_variety(data)    # returns a numbered series\n",
    "test['topic_variety'] = topics_variety(test)    # returns a numbered series\n",
    "num_X['topic_popularity'], test['topic_popularity']= topic_popularity(data, test) # returns a numbered series\n",
    "num_X['topic_citations_avarage'], test['topic_citations_avarage'] = topic_citations_avarage(data, test) # returns a numbered series\n",
    "num_X['venue_popularity'], num_X['venue'], test['venue_popularity'], test['venue'] = venue_popularity(data, test)  # returns a numbered series and a pandas.Series of the 'venues' column reformatted \n",
    "num_X['open_access'] = pd.get_dummies(data[\"is_open_access\"], drop_first = True)  # returns pd.df (True = 1)\n",
    "test['open_access'] = pd.get_dummies(test[\"is_open_access\"], drop_first = True)  # returns pd.df (True = 1)\n",
    "num_X['age'] = age(data)               # returns a numbered series. Needs to be called upon AFTER the venues have been reformed (from venue_frequency)\n",
    "test['age'] = age(test)               # returns a numbered series. Needs to be called upon AFTER the venues have been reformed (from venue_frequency)\n",
    "num_X['venPresL'], test['venPresL'] = venues_citations(data, test)   # returns a numbered series. Needs to be called upon AFTER the venues have been reformed (from venue_frequency)\n",
    "keywords = best_keywords(data, 1, 0.954, 0.955)    # from [data set] get [integer] keywords from papers btw [lower bound] and [upper bound] quantiles; returns list\n",
    "num_X['has_keyword'] = abst_words(data, keywords)#returns a numbered series: 1 if any of the words is present in the abstract, else 0\n",
    "test['has_keyword'] = abst_words(test, keywords)#returns a numbered series: 1 if any of the words is present in the abstract, else 0\n",
    "num_X['keyword_count'] = abst_count(data, keywords) # same as above, only a count (noot bool)\n",
    "test['keyword_count'] = abst_count(test, keywords) # same as above, only a count (noot bool)\n",
    "\n",
    "# Author H-index\n",
    "author_db, data['authors'] = author_database(data)\n",
    "_, test['authors'] = author_database(test) # reformatting authors name from test database\n",
    "num_X['h_index'], test['h_index'] = paper_h_index(data, author_citation_dic, test) # Returns a numbered series. Must come after author names have been reformatted.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "END do not reorder\n",
    "\"\"\"\n",
    "print(\"Features created\")\n",
    "##########################################\n",
    "#    Filling specific missing values     #\n",
    "##########################################\n",
    "missing_values2(num_X)\n",
    "missing_values2(test)\n",
    "      \n",
    "### Drop columns containing just strings\n",
    "num_X = num_X.drop(['authors', 'abstract', 'topics', 'title', 'venue', 'doi', 'fields_of_study'], axis = 1)\n",
    "test = test.drop(['authors', 'abstract', 'topics', 'title', 'venue', 'fields_of_study'], axis = 1)\n",
    "\n",
    "### Drop duplicate rows - this is a lenient definition, but it returns a very small number of rows, so I like it better than a full match \n",
    "duplicate = data[data.duplicated(['title', 'year'])]\n",
    "a = list(duplicate.index)\n",
    "num_X = num_X.drop(labels = a)\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "##########################################\n",
    "#    Outlier detection 1: threshold      #\n",
    "##########################################\n",
    "# 9658 rows in the full num_X\n",
    "# 9494 rows with all turned on\n",
    "\n",
    "num_X = num_X[num_X['references'] < 500]\n",
    "num_X = num_X[num_X['team_sz'] < 40]\n",
    "num_X = num_X[num_X['topic_variety'] < 60]\n",
    "num_X = num_X[num_X['venPresL'] < 300]\n",
    "num_X = num_X[num_X['h_index'] < 30]\n",
    "\n",
    "#%store num_X\n",
    "\n",
    "##########################################\n",
    "#            Train/val split             #\n",
    "##########################################\n",
    "X_train, X_val, y_train, y_val = split_val(num_X, target_variable = 'citations')\n",
    "print(\"Data split\")\n",
    "\n",
    "##########################################\n",
    "#     Outlier detection 2: Quantile      #\n",
    "##########################################\n",
    "### MODEL code for outlier detection\n",
    "\n",
    "# print(list(X_train.columns))\n",
    "\n",
    "out_y = (find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))[0]\n",
    "out_rows = out_y\n",
    "\n",
    "# out_X = (find_outliers_tukey(x = X_train['team_sz'], top = 99, bottom = 0))[0]\n",
    "# out_rows = out_y + out_X\n",
    "\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "X_train = X_train.drop(labels = out_rows)\n",
    "y_train = y_train.drop(labels = out_rows)\n",
    "\n",
    "# Potential features to get rid of: team_sz; year and age are perfect correlates\n",
    "print(\"Outliers handeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df65f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOVE models here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd42f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression r2: 0.010888748824995909\n",
      "LinearRegression MAE: 37.59652310852939\n",
      "\n",
      "log_reg r2: -0.007925469874771274\n",
      "log_reg MAE: 38.28490743646062\n",
      "\n",
      "sdg_reg r2: ('constant', 1, 'squared_error', 6927013070655.014, -1.7567573171967925e+21)\n",
      "sdg_reg r2: ('constant', 0.1, 'squared_error', 276057376659.5687, -7.309941770827021e+18)\n",
      "sdg_reg r2: ('constant', 0.01, 'squared_error', 38.6684034073608, 0.01305571376292336)\n",
      "sdg_reg r2: ('constant', 0.001, 'squared_error', 37.819241514232445, 0.008931169299464803)\n",
      "sdg_reg r2: ('constant', 0.0001, 'squared_error', 37.58700347431105, 0.010726902363878965)\n",
      "sdg_reg r2: ('constant', 1, 'huber', 36.8872786391099, -0.010847379090762388)\n",
      "sdg_reg r2: ('constant', 0.1, 'huber', 36.27849585906639, -0.006382383600121999)\n",
      "sdg_reg r2: ('constant', 0.01, 'huber', 36.32951221974343, -0.0077157832816985294)\n",
      "sdg_reg r2: ('constant', 0.001, 'huber', 36.526557334228436, -0.010979933375479867)\n",
      "sdg_reg r2: ('constant', 0.0001, 'huber', 37.803659632783585, -0.020141096317767948)\n",
      "sdg_reg r2: ('optimal', 1, 'squared_error', 6612.697112641802, -805276.094850363)\n",
      "sdg_reg r2: ('optimal', 0.1, 'squared_error', 6612.697112641802, -805276.094850363)\n",
      "sdg_reg r2: ('optimal', 0.01, 'squared_error', 6612.697112641802, -805276.094850363)\n",
      "sdg_reg r2: ('optimal', 0.001, 'squared_error', 6612.697112641802, -805276.094850363)\n",
      "sdg_reg r2: ('optimal', 0.0001, 'squared_error', 6612.697112641802, -805276.094850363)\n",
      "sdg_reg r2: ('optimal', 1, 'huber', 36.28470789934784, -0.006543545645995019)\n",
      "sdg_reg r2: ('optimal', 0.1, 'huber', 36.28470789934784, -0.006543545645995019)\n",
      "sdg_reg r2: ('optimal', 0.01, 'huber', 36.28470789934784, -0.006543545645995019)\n",
      "sdg_reg r2: ('optimal', 0.001, 'huber', 36.28470789934784, -0.006543545645995019)\n",
      "sdg_reg r2: ('optimal', 0.0001, 'huber', 36.28470789934784, -0.006543545645995019)\n",
      "sdg_reg r2: ('invscaling', 1, 'squared_error', 266615543798.87372, -2.915675366253181e+18)\n",
      "sdg_reg r2: ('invscaling', 0.1, 'squared_error', 37.363869630255735, 0.006784114825693854)\n",
      "sdg_reg r2: ('invscaling', 0.01, 'squared_error', 37.48999629864775, 0.01004060371979476)\n",
      "sdg_reg r2: ('invscaling', 0.001, 'squared_error', 37.594224308680865, 0.010866547086422651)\n",
      "sdg_reg r2: ('invscaling', 0.0001, 'squared_error', 37.562211743817365, 0.01075301086296332)\n",
      "sdg_reg r2: ('invscaling', 1, 'huber', 36.2851966276276, -0.006614562806101398)\n",
      "sdg_reg r2: ('invscaling', 0.1, 'huber', 36.34853531526509, -0.008086602472272997)\n",
      "sdg_reg r2: ('invscaling', 0.01, 'huber', 36.79554642914015, -0.013649038902197264)\n",
      "sdg_reg r2: ('invscaling', 0.001, 'huber', 39.43458448188866, -0.0264878317665882)\n",
      "sdg_reg r2: ('invscaling', 0.0001, 'huber', 41.33598438645798, -0.03158869551193533)\n",
      "poly_reg r2: -0.010680192818456113\n",
      "poly_reg MAE 40.351060444900064\n",
      "pois_reg r2: 0.015325140092279255\n",
      "pois_reg MAE: 37.232232165249\n",
      "kn_reg r2: 0.012851006166082368\n",
      "\n",
      "data unchanged:\n",
      "True\n",
      "True\n",
      "\n",
      "Models complete\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#         Model implementations          #\n",
    "##########################################\n",
    "# It takes 20 to 25 minutes to run all models\n",
    "\n",
    "\n",
    "from CODE.models.regression import simple_linear\n",
    "from CODE.models.regression import log_reg\n",
    "from CODE.models.regression import sdg_reg\n",
    "from CODE.models.regression import poly_reg\n",
    "from CODE.models.regression import pois_reg\n",
    "from CODE.models.non_linear import de_tree_reg\n",
    "from CODE.models.non_linear import kn_reg\n",
    "from CODE.models.non_linear import my_svr\n",
    "from CODE.models.non_linear import mlp_reg\n",
    "\"\"\"\n",
    "IMPLEMENT models here: to run a model, delete the # and run\n",
    "NOTE: Please do not modify X_train, X_val, y_train, y_val in your model - make new variables if needed\n",
    "\"\"\"\n",
    "\n",
    "#----------- Check for changes\n",
    "check_X = X_train.copy(deep = True)\n",
    "check_y = y_train.copy(deep = True)\n",
    "\n",
    "#-----------simple regression, all columns\n",
    "# Leave this on as a baseline\n",
    "simple_linear(X_train, y_train, X_val, y_val)\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "R2: 0.03724 \n",
    "MSE: 33.38996\n",
    "# Worse after extra outlier removal (0.015478)\n",
    "\"\"\"\n",
    "#-----------logistic regression, all columns\n",
    "log_reg(X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "R2: 0.006551953988217396\n",
    "MSE: 34.07342328208346\n",
    "# Worse after extra outlier removal (0.003)\n",
    "\"\"\"\n",
    "#-----------SGD regression, all columns\n",
    "sdg_reg (X_train, y_train, X_val, y_val)\n",
    "\n",
    "\"\"\"\n",
    "lr = [ 1, .1, .01, .001, .0001]\n",
    "learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "loss in ['squared_error', 'huber']:\n",
    "\n",
    "# MODEL RESULTS:\n",
    "# Best outcome, before extra outlier removal: ('constant', 0.01, 'squared_error', 35.74249957361433, 0.04476790061780822)\n",
    "# Best outcome after extra outlier removal: ('constant', 0.01, 'squared_error', 37.08290449479669, 0.019303736163186702)\n",
    "\"\"\"\n",
    "\n",
    "#-----------polynomial regression, all columns\n",
    "poly_reg (X_train, y_train, X_val, y_val, 3)\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "r2: -0.05109 (degree = 2)\n",
    "r2: -0.0378 (degree = 3)\n",
    "r2: -5.5816 (degree = 4)\n",
    "MAE 35.1660\n",
    "\"\"\"\n",
    "\n",
    "#-----------poisson regression, all columns\n",
    "pois_reg (X_train, y_train, X_val, y_val)\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "r2: 0.022145\n",
    "MAE: 39.21127\n",
    "\"\"\"\n",
    "\n",
    "#-----------simple linear regression, dropping columns\n",
    "\n",
    "\"\"\"\n",
    "USE this code to run one of the simple regression models, successively dropping one column\n",
    "To run, unhash the full function, then unhash the specific model\n",
    "For a baseline, run the corresponding model above\n",
    "\"\"\"\n",
    "# summaries = list(X_train.columns)\n",
    "# print(summaries)\n",
    "\n",
    "# for i in range(len(summaries)):\n",
    "#     X_train_small = X_train.copy()\n",
    "#     X_val_small = X_val.copy()\n",
    "#     drops = summaries[i]\n",
    "#     X_train_small.drop(drops, inplace = True, axis=1)\n",
    "#     X_val_small.drop(drops, inplace = True, axis=1)\n",
    "\n",
    "#     print(\"dropped:\", summaries[i])\n",
    "    \n",
    "#     #simple_linear(X_train_small, y_train, X_val_small, y_val)  #dropping venue_popularity helps a tiny bit\n",
    "#     #log_reg(X_train_small, y_train, X_val_small, y_val)\n",
    "\n",
    "\n",
    "#----------- Random Forrest for Regression\n",
    "# +/- 5 min to run\n",
    "#de_tree_reg (X_train, y_train, X_val, y_val, 50)\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "r2: 0.03378200504507167\n",
    "\"\"\"\n",
    "\n",
    "#----------- K-Neighbors for Regression\n",
    "kn_reg (X_train, y_train, X_val, y_val)\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "r2: 0.016964846143673884\n",
    "\"\"\"\n",
    "\n",
    "#----------- SVR\n",
    "# from sklearn.svm import SVR\n",
    "# svr = SVR()\n",
    "# model = svr.fit(X_train, np.ravel(y_train))\n",
    "# r_sq1 = model.score(X_val, y_val)\n",
    "# print('r2 scr:', r_sq1)\n",
    "# print()\n",
    "\n",
    "\n",
    "#-----------  Multi-layer Perceptron for Regression\n",
    "#mlp_reg (X_train, y_train, X_val, y_val, maxit=1000, activation='relu', solver='adam', alpha=0.0001, lr='constant') \n",
    "\n",
    "\"\"\"\n",
    "OPTIONS:\n",
    "activation= 'identity', 'logistic', 'tanh', 'relu'\n",
    "solver= 'lbfgs', 'sgd', 'adam'\n",
    "lr= 'constant', 'invscaling', 'adaptive'\n",
    "DEFAULT values: maxit=500, activation='relu', solver='adam', alpha=0.0001, lr='constant'\n",
    "\n",
    "MODEL RESULTS:\n",
    "r2: 0.005729150866153665\n",
    "score: 0.005729150866153665\n",
    "\"\"\"\n",
    "\n",
    "print(\"data unchanged:\")\n",
    "print(check_X.equals(X_train))\n",
    "print(check_y.equals(y_train))\n",
    "print()\n",
    "print(\"Models complete\")\n",
    "\n",
    "\n",
    "# ##########################################\n",
    "# #  Writing file with predicted values    #\n",
    "# ##########################################\n",
    "# \"\"\"\n",
    "#     Creates new DataFrame with DOI of the papers, \n",
    "#     and predicted citation values.\n",
    "# \"\"\"\n",
    "\n",
    "# df_output = pd.DataFrame(columns = ['doi','citations'])\n",
    "\n",
    "# dict_output = {}\n",
    "\n",
    "# y_test = model.predict(test.drop(['doi'], axis=1))\n",
    "# for index, i_paper in test.iterrows():\n",
    "#     df_output.loc[index, 'doi'] = i_paper['doi'] \n",
    "#     df_output.loc[index, 'citations'] = y_test[index]\n",
    "\n",
    "# list_dic_output = df_output.to_dict(orient = 'records')\n",
    "\n",
    "# import json\n",
    "\n",
    "# jsonOutput = json.dumps(list_dic_output, indent = 4)\n",
    "# with open('OUTPUT/predicted.json', 'w') as f:\n",
    "#     json.dump(jsonOutput, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0219f-cab9-48a4-86f8-bd4be754b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------------ EXPLORE and VISUALIZE ------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ab51f-d954-42fa-bf20-80ea54b6757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STOP - ONLY if needed\n",
    "# Allows printing full text\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04915fc1-7323-404b-aa26-761a7b9ee7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: exploring the new dataframe with numerical columns\n",
    "num_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many na?\n",
    "#sum([True for i, row in num_X.iterrows() if any(row.isnull())])\n",
    "print(data.shape[0] - data.dropna().shape[0])\n",
    "print(data['year'].isna().sum())\n",
    "print(data['references'].isna().sum())\n",
    "print(data['is_open_access'].isna().sum())\n",
    "\n",
    "print(data['year'].isna().sum())\n",
    "print(data['references'].isna().sum())\n",
    "print(data['is_open_access'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90346f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per column: min, max, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394af11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828176-3b43-45e1-98ab-de2e109ee06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look at some correlations - full num_X\n",
    "\"\"\"\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "\n",
    "# From: https://www.kaggle.com/ankitjha/comparing-regression-models\n",
    "import seaborn as sns\n",
    "corr_mat = num_X.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be5a4a-20e6-45db-a477-548a15040c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look at some correlations - X_train\n",
    "NOTE: there is no y here\n",
    "\"\"\"\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "\n",
    "#temp = y_train hstack X_train\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/ankitjha/comparing-regression-models\n",
    "corr_mat = X_train.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots of each column against y\n",
    "\"\"\"\n",
    "import matplotlib as plt\n",
    "num_X.plot.scatter(x=\"year\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"references\", y=\"citations\", alpha=0.5)  # might have 3 outliers\n",
    "num_X.plot.scatter(x=\"title_length\", y=\"citations\", alpha=0.5) # anything over 30 as outlier?\n",
    "num_X.plot.scatter(x=\"team_sz\", y=\"citations\", alpha=0.5)  # might have 3 outliers\n",
    "num_X.plot.scatter(x=\"topic_var\", y=\"citations\", alpha=0.5) # one outlier; maybe anything over 40\n",
    "num_X.plot.scatter(x=\"topic_popularity\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"topic_citations_average\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"venue_popularity\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"open_access\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"age\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"venPresL\", y=\"citations\", alpha=0.5)  # anything over 300 as outlier?\n",
    "num_X.plot.scatter(x=\"has_keyword\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"keyword_count\", y=\"citations\", alpha=0.5)\n",
    "num_X.plot.scatter(x=\"h_index\", y=\"citations\", alpha=0.5)  # anything over 35 as outlier?\n",
    "num_X.plot.scatter(x=\"field_cit\", y=\"citations\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per column histogram, heatmap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a982b8-4ac9-41e1-bfe2-5265e574b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR: explore keyword generation\n",
    "# names: X_train, X_val, y_train, y_val\n",
    "print(\"number of keywords:\", len(keywords))\n",
    "print(\"total train rows:\", X_train.shape)\n",
    "print(\"numer w keyword:\", sum(X_train['has_keyword']))\n",
    "print()\n",
    "print(keywords)\n",
    "\n",
    "# Results are pretty useless as is\n",
    "#6210 of 6313\n",
    "#6136 (of 6313) for 1 keyword from the top 1% of papers\n",
    "#4787 for 2 keywords from top .01% of papers (correlation: 0.036)\n",
    "#2917 for 1 keyword from top .01% of papers (correlation: 0.049)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c35830-e499-4409-a81b-e0fc78404744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "------------------------- TEST CODE -----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f4baa-0dc1-4730-8d99-334b8206b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a play copy of the training data\n",
    "\"\"\"\n",
    "play = X_train.copy()\n",
    "print(list(X_train.columns))\n",
    "print(X_train.shape)\n",
    "print(play.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2820d4b-4200-4214-bc23-27c872ae6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a random mini version of the main 'data' dataframe\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "mini = data.sample(100, replace = False, axis = 0, random_state = 123)  \n",
    "print(mini.shape)\n",
    "print(list(mini.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b1c3b-c2c7-4fff-bfdb-7bb7335aceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose your columns\n",
    "\"\"\"\n",
    "\n",
    "#X_train_small = X_train.loc[ : , 'topic_var':'h_index'].copy()\n",
    "#X_val_small = X_val.loc[ : , 'topic_var':'h_index'].copy()\n",
    "\n",
    "drops = ['year', 'team_sz', 'has_keyword']\n",
    "X_train_small = X_train.copy()\n",
    "X_train_small.drop(drops, inplace = True, axis=1)\n",
    "\n",
    "X_val_small = X_val.copy()\n",
    "X_val_small.drop(drops, inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c78afa-373a-41ed-8080-78d7eea535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This doesn't work: there are a different number of words in each list.\n",
    "Maybe just take the top... 20?\n",
    "\"\"\"\n",
    "\n",
    "def abst_categories (the_data, keywords, mid_keywords, low_keywords):\n",
    "    abst = the_data['abstract']\n",
    "    counts = []\n",
    "    abst_key = []    \n",
    "\n",
    "    for i in abst:\n",
    "        if i == None:\n",
    "            abst_key.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            high = 0\n",
    "            for word in keywords:\n",
    "                if word in i.lower():\n",
    "                    high += 1\n",
    "            \n",
    "            mid = 0\n",
    "            for word in mid_keywords:\n",
    "                if word in i.lower():\n",
    "                    mid += 1\n",
    "\n",
    "            low = 0\n",
    "            for word in low_keywords:\n",
    "                if word in i.lower():\n",
    "                    low +=1\n",
    "       \n",
    "#        abst_key = np.argmax(abst_key)\n",
    "#        abst_key = (max(abst_key)).index\n",
    "\n",
    "    return pd.Series(abst_key)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a0f77-2b2e-41e5-a9fa-84705bf7fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "\n",
    "model = PoissonRegressor()\n",
    "reg = model.fit(X = X_train_s, y = y_train)\n",
    "y_pred_val = reg.predict(X_val_s)\n",
    "\n",
    "print('r2:', r2_score(y_val, y_pred_val))\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred_val))\n",
    "\n",
    "\"\"\"\n",
    "MODEL RESULTS:\n",
    "r2: 0.022145\n",
    "MAE: 39.21127\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sklearn:\n",
    "SVR\n",
    "KNeighborsRegressor()\n",
    "DecisionTreeRegression()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC  - classifier\n",
    "#... load the data into X,y\n",
    "model = SVC(kernel='poly')\n",
    "model.fit(X,y)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
