{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf4728-fa09-436d-83b3-aed33d5c7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alt method for keyword creation\n",
    "\n",
    "# source: https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c\n",
    "import yake\n",
    "\n",
    "def best_keywords (data, words_per_paper):\n",
    "    a = data['citations'].quantile(q = .9)\n",
    "    best = data[data['citations'] > a]\n",
    "    best['keywords'] = ''\n",
    "\n",
    "    extractor = yake.KeywordExtractor()\n",
    "    language = 'en'\n",
    "    max_ngram = 1\n",
    "    deduplication_threshold = 0.1   # low value = duplication of keywords in ngrams not allowed\n",
    "    num_keywords = words_per_paper\n",
    "\n",
    "\n",
    "    for index, i_paper in best.iterrows(): # iterate over the dataframe \n",
    "        text = i_paper['abstract']\n",
    "        custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram, dedupLim=deduplication_threshold, \n",
    "                                                    top=num_keywords, features=None)\n",
    "        keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "        k = []\n",
    "        for i in keywords:\n",
    "            k.append(i[0].lower())\n",
    "\n",
    "        best.at[index, 'keywords'] = k\n",
    "\n",
    "    a = best['keywords'].tolist()\n",
    "    a = [x for l in a for x in l]\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "print(best_keywords(play, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove outliers\n",
    "NOTE: can't rerun this code without restarting the kernal\n",
    "\"\"\"\n",
    "#names: X_train, X_val, y_train, y_val\n",
    "#print(list(X_train.columns))\n",
    "\n",
    "# print(\"citations:\", find_outliers_tukey(x = y_train['citations'], top = 93, bottom = 0))\n",
    "\n",
    "# print(\"year:\", find_outliers_tukey(X_train['year'], top = 74, bottom = 25))  # seems unnecessary\n",
    "# print(\"references:\", find_outliers_tukey(X_train['references'], top = 90, bottom = 10))  # seems unnecessary\n",
    "# print(\"team_size:\", find_outliers_tukey(X_train['team_size'], top = 99, bottom = 0))  # Meh\n",
    "# print(\"topic_variety:\", find_outliers_tukey(X_train['topic_variety'], top = 75, bottom = 10))  # not much diff btw top and normal\n",
    "# print(\"age:\", find_outliers_tukey(X_train['age'], top = 90, bottom = 10))  # Meh\n",
    "# print(\"open_access:\", find_outliers_tukey(X_train['open_access'], top = 100, bottom = 0))  # Not necessary: boolean\n",
    "# print(\"has_keyword:\", find_outliers_tukey(X_train['has_keyword'], top = 100, bottom = 0))  # Not necessary: boolean\n",
    "# print(\"title_length:\", find_outliers_tukey(X_train['title_length'], top = 90, bottom = 10))  # Meh\n",
    "# print(\"field_variety:\", find_outliers_tukey(X_train['field_variety'], top = 90, bottom = 10))  # seems unnecessary\n",
    "# print(\"venue_freq:\", find_outliers_tukey(X_train['venue_freq'], top = 90, bottom = 10))  # seems unnecessary\n",
    "\n",
    "\n",
    "out_y = (find_outliers_tukey(x = y_train['citations'], top = 95, bottom = 0))[0]\n",
    "#out_X = (find_outliers_tukey(x = X_train['team_size'], top = 99, bottom = 0))[0]\n",
    "out_rows = out_y\n",
    "#out_rows = out_y + out_X\n",
    "out_rows = sorted(list(set(out_rows)))\n",
    "\n",
    "print(\"X_train:\")\n",
    "print(X_train.shape)\n",
    "X_train = X_train.drop(labels = out_rows)\n",
    "print(X_train.shape)\n",
    "print()\n",
    "print(\"y_train:\")\n",
    "print(y_train.shape)\n",
    "y_train = y_train.drop(labels = out_rows)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2785ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train_small)\n",
    "X_val_z  =scaler.transform(X_val_small)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree = 2)\n",
    "x_train_poly = polynomial_features.fit_transform(X_train_z)\n",
    "x_val_poly = polynomial_features.transform(X_val_z)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_poly, y_train)\n",
    "y_poly_pred = model.predict(x_val_poly)\n",
    "\n",
    "print(r2_score(y_val, y_poly_pred))   # -0.04350391168707901\n",
    "print(mean_absolute_error(y_val, y_poly_pred))    # 32.65668266590838"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
