{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf4728-fa09-436d-83b3-aed33d5c7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alt method for keyword creation\n",
    "\n",
    "# source: https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c\n",
    "import yake\n",
    "\n",
    "def best_keywords (data, words_per_paper):\n",
    "    a = data['citations'].quantile(q = .9)\n",
    "    best = data[data['citations'] > a]\n",
    "    best['keywords'] = ''\n",
    "\n",
    "    extractor = yake.KeywordExtractor()\n",
    "    language = 'en'\n",
    "    max_ngram = 1\n",
    "    deduplication_threshold = 0.1   # low value = duplication of keywords in ngrams not allowed\n",
    "    num_keywords = words_per_paper\n",
    "\n",
    "\n",
    "    for index, i_paper in best.iterrows(): # iterate over the dataframe \n",
    "        text = i_paper['abstract']\n",
    "        custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram, dedupLim=deduplication_threshold, \n",
    "                                                    top=num_keywords, features=None)\n",
    "        keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "        k = []\n",
    "        for i in keywords:\n",
    "            k.append(i[0].lower())\n",
    "\n",
    "        best.at[index, 'keywords'] = k\n",
    "\n",
    "    a = best['keywords'].tolist()\n",
    "    a = [x for l in a for x in l]\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "print(best_keywords(play, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "#         Model implementations          #\n",
    "##########################################\n",
    "from CODE.models.regression import simple_linear\n",
    "from CODE.models.regression import log_reg\n",
    "from CODE.models.regression import sdg_reg\n",
    "\n",
    "\"\"\"\n",
    "IMPLEMENT models here\n",
    "NOTE: Please do not modify X_train, X_val, y_train, y_val in your model - make new variables if needed\n",
    "\"\"\"\n",
    "\n",
    "#-----------simple regression, all columns\n",
    "\"\"\"\n",
    "simple_linear(X_train, y_train, X_val, y_val)\n",
    "\n",
    "MODEL RESULTS:\n",
    "R2: 0.03724\n",
    "MSE: 33.38996\n",
    "\"\"\"\n",
    "#-----------logistic regression, all columns\n",
    "\"\"\"\n",
    "log_reg(X_train, y_train, X_val, y_val)\n",
    "\n",
    "MODEL RESULTS:\n",
    "R2: 0.006551953988217396\n",
    "MSE: 34.07342328208346\n",
    "\"\"\"\n",
    "#-----------SGD regression, all columns\n",
    "#sdg_reg (X_train, y_train, X_val, y_val)\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.transform(X_val)\n",
    "y_ravel = np.ravel(y_train)\n",
    "lr = [ 1, .1, .01, .001, .0001]\n",
    "settings = []\n",
    "for learning_rate in ['constant', 'optimal', 'invscaling']:\n",
    "    for loss in ['squared_error', 'huber']:\n",
    "        for eta0 in lr:\n",
    "            model = SGDRegressor(learning_rate=learning_rate, eta0=eta0, loss=loss,random_state=666, max_iter=5000)\n",
    "            model.fit(X_train_z, y_ravel)\n",
    "            y_pred = model.predict(X_val_z)\n",
    "            \n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 =  r2_score(y_val, y_pred)\n",
    "            settings.append((learning_rate, eta0, loss, mae, r2))\n",
    "            print(settings[-1])\n",
    "\n",
    "# MODEL RESULTS:\n",
    "# Best outcome: ('constant', 0.01, 'squared_error', 35.74249957361433, 0.04476790061780822)\n",
    "\"\"\"\n",
    "\n",
    "#-----------polynomial regression, all columns\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z  =scaler.transform(X_val)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree = 2)\n",
    "x_train_poly = polynomial_features.fit_transform(X_train_z)\n",
    "x_val_poly = polynomial_features.transform(X_val_z)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_poly, y_train)\n",
    "y_poly_pred = model.predict(x_val_poly)\n",
    "\n",
    "print(r2_score(y_val, y_poly_pred))   # -0.04350391168707901\n",
    "print(mean_absolute_error(y_val, y_poly_pred))    # 32.65668266590838\n",
    "\n",
    "source: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.fit(X_train, y_train)\n",
    "#print('Best score: ', model.best_score_)\n",
    "#print('Best parameters: ', model.best_params_)\n",
    "#y_pred = model.predict(X_val)\n",
    "\n",
    "#from sklearn.metrics import r2_score\n",
    "#print(r2_score(y_val,y_pred))\n",
    "\n",
    "\n",
    "# import json\n",
    "#with open(\"sample.json\", \"w\") as outfile:\n",
    "    #json.dump(dictionary, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
